{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpD2bVdSQu_v",
        "outputId": "5555fb40-081c-4cda-89d0-f775d48dbe6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/Winter '22/IR/Project/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy5hm0NIlzfB",
        "outputId": "8b8d9a49-4330-440c-beab-1a2caf3fc9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " aspectWordMat\t\t    final_df_withsim500K.sav   nb_sim250k.sav\n",
            " aspectWordMat_json\t    final_df_withsim50k.sav    nb_sim500K.sav\n",
            " DT_sim250k.sav\t\t    final_df_withsim.csv       nb_sim.sav\n",
            " DT_sim500K.sav\t\t    final_df_withsim.sav       nb_withoutsim500K.sav\n",
            " DT_sim.sav\t\t    Full_HotelRec.zip\t       ProcessedText.txt\n",
            " FetchedRecords.txt\t    IR_Proj.ipynb\t       RForest_sim250k.sav\n",
            " final_df250k.sav\t    KNN.sav\t\t       RForest_sim500K.sav\n",
            " final_df500k.sav\t    KNN_sim250k.sav\t       RForest_sim.sav\n",
            " final_df500K.sav\t    KNN_sim500K.sav\t       rp166f-wang.pdf\n",
            " final_df50k.sav\t    KNN_sim.sav\t\t       svm_sim250k.sav\n",
            " final_df.csv\t\t    List_for_ranking100K.sav   svm_sim.sav\n",
            " final_df.sav\t\t    Logistic_sim250k.sav       svm_withoutsim500K.sav\n",
            " final_df_withsim100K.sav   Logistic_sim500K.sav       UniqueHotels.pkl\n",
            " final_df_withsim250k.sav   Logistic_sim.sav\t      'Wang_LARA DFo7R.pdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fpx3yvLwaVSB",
        "outputId": "389ab23f-d72f-4fcd-faa1-dd97b93b2f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500000\n"
          ]
        }
      ],
      "source": [
        "from itertools import islice\n",
        "\n",
        "PATH = \"/content/drive/My Drive/Winter '22/IR/Project/ProcessedText.txt\"\n",
        "\n",
        "N = 500000\n",
        "with open(PATH,\"r\") as myfile:\n",
        "    record_list = list(islice(myfile, N))\n",
        "print(len(record_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_nSEZJ4Iu-Y"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "rev=\"\"\n",
        "all_rev=[]\n",
        "all_rev_txt=[]\n",
        "all_rev_rating=[]\n",
        "all_rev_id=[]\n",
        "\n",
        "for r in range(len(record_list)):\n",
        "  json_data=json.loads(record_list[r])\n",
        "  rev+=json_data['text']\n",
        "  all_rev.append(json_data)\n",
        "  all_rev_txt.append(json_data['text'])\n",
        "  all_rev_rating.append(json_data['property_dict'])\n",
        "\n",
        "#Clearing memory\n",
        "record_list = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHD0HSe_R1jY",
        "outputId": "00764d7f-0935-4c1d-b4ee-f9668d9ef2eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPAq3o1SQ3cm",
        "outputId": "d3c94b3d-c887-47a4-c0e1-92836cabd6b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3747557\n"
          ]
        }
      ],
      "source": [
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "sentences=[]\n",
        "review_index=[]\n",
        "\n",
        "ind=0\n",
        "for review in all_rev_txt:\n",
        "  temp1=review.split('.')\n",
        "  temp2=[]\n",
        "  #print(temp1)\n",
        "  for s in temp1:\n",
        "    sentence=s.lower()\n",
        "    words=word_tokenize(s)\n",
        "    en_stopwords = set(stopwords.words('english'))\n",
        "    processed_words=[]\n",
        "    for token in words:\n",
        "          if token not in en_stopwords:\n",
        "              processed_words.append(token)\n",
        "\n",
        "    tokenizer= RegexpTokenizer(r\"\\w+\")\n",
        "    word_list=tokenizer.tokenize(' '.join(processed_words))\n",
        "    word_list=[w.lower() for w in word_list if len(w)!=1]\n",
        "    temp2.append(word_list)\n",
        "\n",
        "  temp3=[]\n",
        "  for s in temp2:\n",
        "    if(len(s)!=0):\n",
        "      temp3.append(' '.join(s))\n",
        "      review_index.append(ind)\n",
        "\n",
        "  sentences+=temp3\n",
        "  ind+=1\n",
        "\n",
        "#sentences\n",
        "print(len(sentences))\n",
        "\n",
        "#Clearing Memory\n",
        "all_rev=None\n",
        "all_rev_txt=None\n",
        "all_rev_rating=None\n",
        "all_rev_id=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jffsENB3ah0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8403c444-b561-45dc-e0cd-7c4283e6bf84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#sentences=nltk.sent_tokenize(rev)\n",
        "sentence_list=[]\n",
        "for s in sentences:\n",
        "  sentence=s.lower()\n",
        "  words=word_tokenize(s)\n",
        "  en_stopwords = set(stopwords.words('english'))\n",
        "  processed_words=[]\n",
        "  for token in words:\n",
        "        if token not in en_stopwords:\n",
        "            processed_words.append(token)\n",
        "\n",
        "  tokenizer= RegexpTokenizer(r\"\\w+\")\n",
        "  word_list=tokenizer.tokenize(' '.join(processed_words))\n",
        "  word_list=[w.lower() for w in word_list if len(w)!=1]\n",
        "  sentence_list.append(word_list)\n",
        "\n",
        "sentences=[]\n",
        "for s in sentence_list:\n",
        "  if(len(s)!=0):\n",
        "    sentences.append(' '.join(s))\n",
        "\n",
        "sentence_wordfreq=[]\n",
        "\n",
        "for i in sentence_list:\n",
        "  sentence_wordfreq.append(FreqDist(i))\n",
        "\n",
        "print(sentence_wordfreq)\n",
        "\n",
        "#Clearing Memory\n",
        "sentence_list = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "583Kipf8ttT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab70c155-df82-4a80-edbc-e10ee567f0b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(None, {'value': ['value', 'price'], 'room': ['room', 'space'], 'location': ['location', 'locate'], 'cleanliness': ['clean', 'dirty'], 'service': ['service', 'manager']})\n"
          ]
        }
      ],
      "source": [
        "from typing import DefaultDict\n",
        "aspect_keyword=DefaultDict()\n",
        "\n",
        "aspect_keyword[\"value\"]=[\"value\",\"price\"]\n",
        "aspect_keyword[\"room\"]=[\"room\",\"space\"]\n",
        "aspect_keyword[\"location\"]=[\"location\",\"locate\"]\n",
        "aspect_keyword[\"cleanliness\"]=[\"clean\",\"dirty\"]\n",
        "aspect_keyword[\"service\"]=[\"service\",\"manager\"]\n",
        "\n",
        "print(aspect_keyword)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w05ht5ltlaz"
      },
      "source": [
        "functions needed for aspectsegmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1W8WflRsUrd"
      },
      "outputs": [],
      "source": [
        "all_words=[]\n",
        "\n",
        "for s in sentences:\n",
        "  for w in s.split():\n",
        "    if w not in all_words:\n",
        "      all_words.append(w)\n",
        "\n",
        "all_words = list(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn_lNIRvshub"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def assign_aspect(wordfreq):\n",
        "  sentence_aspect=[]\n",
        "  count=DefaultDict(int)\n",
        "  for word in wordfreq.keys():\n",
        "    for aspect, keywords in aspect_keyword.items():\n",
        "      if word in keywords:\n",
        "        count[aspect]+=1\n",
        "  if count : \n",
        "    maxi = max(count.values())\n",
        "    for aspect, cnt in count.items():\n",
        "      if cnt==maxi:\n",
        "        sentence_aspect.append(aspect)\n",
        "\n",
        "  return sentence_aspect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWd1F7T-XlMa"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "sentences_dup=copy.deepcopy(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnLLWQPRTKwN"
      },
      "source": [
        "Aspect Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiNacYQcTJNl"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "import heapq\n",
        "import copy\n",
        "\n",
        "aspectWordMat = defaultdict(lambda: defaultdict(int))\n",
        "aspectCount= DefaultDict(int)\n",
        "wordCount=DefaultDict(int)\n",
        "\n",
        "i=1000\n",
        "changed=True\n",
        "while changed:\n",
        "  i-=1\n",
        "  sentence_aspect=[]\n",
        "  for word_freq in sentence_wordfreq:\n",
        "    sentence_aspect.append(assign_aspect(word_freq))\n",
        "\n",
        "  aspect_to_sentence=DefaultDict(list)\n",
        "  aspect_to_sentence.clear()\n",
        "  for i in range(len(sentences)):\n",
        "    if len(sentence_aspect[i])==1:\n",
        "      aspect_to_sentence[sentence_aspect[i][0]].append(sentences[i])\n",
        "\n",
        "  #print(aspect_to_sentence)\n",
        "\n",
        "  for aspect, sentences in aspect_to_sentence.items():\n",
        "              for i in range(len(sentences)):\n",
        "                  for word,freq in sentence_wordfreq[i].items():\n",
        "                      aspectWordMat[aspect][word]+=freq\n",
        "                      aspectCount[aspect]+=freq\n",
        "                      wordCount[word]+=freq\n",
        "                      #print(aspect,\"-\",word)\n",
        "\n",
        "\n",
        "  top_p_words = {}\n",
        "\n",
        "  for aspect in aspect_keyword.keys():\n",
        "    top_p_words[aspect] = []\n",
        "  for word in all_words:\n",
        "      maxChi = 0.0 \n",
        "      maxAspect = \"\" \n",
        "      for aspect in aspect_keyword.keys():\n",
        "        C= sum(aspectCount.values())\n",
        "        C1 = aspectWordMat[aspect][word]\n",
        "        C2 = wordCount[word]-C1\n",
        "        C3 = aspectCount[aspect]-C1 \n",
        "        C4 = C-C1 \n",
        "        deno = (C1+C3)*(C2+C4)*(C1+C2)*(C3+C4)\n",
        "        if deno!=0:\n",
        "          chisq = (C*(C1*C4 - C2*C3)*(C1*C4 - C2*C3))/deno\n",
        "        else:\n",
        "          chisq = 0.0\n",
        "\n",
        "        aspectWordMat[aspect][word] = chisq\n",
        "        if aspectWordMat[aspect][word] > maxChi:\n",
        "          maxChi = aspectWordMat[aspect][word]\n",
        "          maxAspect = aspect\n",
        "      if maxAspect!=\"\":\n",
        "        top_p_words[maxAspect].append((maxChi, word))\n",
        "\n",
        "  changed=False\n",
        "  top_p_words_copy=copy.deepcopy(top_p_words)\n",
        "  for aspect in aspect_keyword.keys():\n",
        "    max_words=[]\n",
        "    for i in range(5):\n",
        "      max_word=\"\"\n",
        "      max_val=0\n",
        "      max_tup=None\n",
        "      for tup in top_p_words_copy[aspect]:\n",
        "        if tup[0]>max_val:\n",
        "          max_val=tup[0]\n",
        "          max_word=tup[1]\n",
        "          max_tup=tup\n",
        "      max_words.append(max_word)\n",
        "      try:\n",
        "        top_p_words_copy[aspect].remove(max_tup)\n",
        "      except:\n",
        "        pass\n",
        "    \n",
        "    for word in max_words:\n",
        "      if word not in aspect_keyword[aspect]:\n",
        "        changed=True\n",
        "        aspect_keyword[aspect].append(word)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "#aspect_keyword"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(aspect_keyword.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-ydKXM7uohX",
        "outputId": "786edb85-2f7c-4355-ff5b-6dfb6ff285e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['value', 'room', 'location', 'cleanliness', 'service'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for aspect in aspect_keyword:\n",
        "  i+=1\n",
        "  if i==8:\n",
        "    break\n",
        "  print(aspect, \" : \", aspect_keyword[aspect][0:7])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XkLyuLK7giE",
        "outputId": "19f24954-502d-488b-b31c-e640c3a75302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value  :  ['value', 'price', 'riad', 'kayumanis', 'kop', 'montparnasse', 'williams']\n",
            "room  :  ['room', 'space', 'hotel', 'disneyland', 'mahana', 'starhill', 'cane']\n",
            "location  :  ['location', 'locate', 'season', 'view', 'ceylin', 'aparts', 'samui']\n",
            "cleanliness  :  ['clean', 'dirty', 'hyatt', 'convention', 'kayana', 'gb', 'movie']\n",
            "service  :  ['service', 'manager', 'villa', 'toberua', 'gowrie', 'asheville', 'romeo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n"
      ],
      "metadata": {
        "id": "sRD9APeSLQJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXl-sTmIhole"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "sentences=[]\n",
        "aspects=[]\n",
        "reviewId=[]\n",
        "sentiments=[]\n",
        "\n",
        "for i in range(len(sentence_aspect)):\n",
        "  if len(sentence_aspect[i])!=0:\n",
        "    try:\n",
        "      sentences.append(sentences_dup[i])\n",
        "      sentiments.append(TextBlob(sentences_dup[i]).sentiment.polarity)\n",
        "      try:\n",
        "        aspects.append(sentence_aspect[i][0])    \n",
        "      except Exception as e:\n",
        "        print(e, \" Skipping...\")\n",
        "      \n",
        "      reviewId.append(review_index[i])\n",
        "    except:\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentences_dup))\n",
        "print(len(sentences))\n",
        "print(len(sentence_aspect))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7Q3Gd6hMpk6",
        "outputId": "502c029f-12ca-4baa-e428-d3dc0270417d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3747547\n",
            "1510362\n",
            "3747557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6FXVRX6nTWf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "final_df_list=[]\n",
        "for i in range(len(sentences)):\n",
        "  temp=[reviewId[i],sentences[i],aspects[i],sentiments[i]]\n",
        "  final_df_list.append(temp)\n",
        "\n",
        "final_df=pd.DataFrame(final_df_list,columns=['ReviewId','sentence','aspect','sentiment score'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(final_df, \"drive/My Drive/Winter '22/IR/Project/final_df500K.sav\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9AjDwzT0k4i",
        "outputId": "f7cb17b0-b9c0-4e93-9f01-44e6771773ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"drive/My Drive/Winter '22/IR/Project/final_df500K.sav\"]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = joblib.load(\"drive/My Drive/Winter '22/IR/Project/final_df500K.sav\")"
      ],
      "metadata": {
        "id": "8w4Ga0FPQaTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final_df.to_csv('final_df.csv')\n",
        "#!cp final_df.csv \"drive/My Drive/Winter '22/IR/Project/\""
      ],
      "metadata": {
        "id": "F0ggtk27I8RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head()"
      ],
      "metadata": {
        "id": "kNNycIUQLVmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiJ4hXX5pxJa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( final_df['sentence'], final_df['aspect'], test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clearing memory\n",
        "final_df = None"
      ],
      "metadata": {
        "id": "ql8V82FeLU3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-uNwGSLXpxV"
      },
      "source": [
        "naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOlq43bsTWa0"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(precision_recall_fscore_support(y_test, y_pred, average='macro'))\n",
        "#print(classification_report(y_test, y_pred,target_names=my_tags))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(nb, \"drive/My Drive/Winter '22/IR/Project/nb_withoutsim500K.sav\")"
      ],
      "metadata": {
        "id": "73z3yAvoNsdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcj3X2zDXsbH"
      },
      "source": [
        "svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-v8um-DUvhF"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', svm.SVC(decision_function_shape='ovr')),\n",
        "              ])\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "print(precision_recall_fscore_support(y_test, y_pred, average='macro'))\n",
        "#print(classification_report(y_test, y_pred,target_names=my_tags))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(nb, \"drive/My Drive/Winter '22/IR/Project/svm_withoutsim500K.sav\")"
      ],
      "metadata": {
        "id": "wquuXERVNytc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = None\n",
        "X_test = None\n",
        "y_train = None\n",
        "y_test = None"
      ],
      "metadata": {
        "id": "8UgOJKw4-0Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EplSmg6ug3_Y"
      },
      "outputs": [],
      "source": [
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH4agQALXyF8"
      },
      "source": [
        "with cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKoMmDThVg0u"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "aspects_with_cosinesim=[]\n",
        "\n",
        "for s in sentences:\n",
        "  for aspect in aspect_keyword.keys():\n",
        "\n",
        "    max_sim=0\n",
        "    maxsim_aspect=\"\"\n",
        "    \n",
        "    X_set = set(aspect_keyword[aspect]) \n",
        "    Y_set = set(s.split())\n",
        "    l1 =[]\n",
        "    l2 =[]\n",
        "     \n",
        "    rvector = X_set.union(Y_set) \n",
        "    for w in rvector:\n",
        "        if w in X_set: \n",
        "          l1.append(1)\n",
        "        else:\n",
        "          l1.append(0) \n",
        "        if w in Y_set: \n",
        "          l2.append(1)\n",
        "        else:\n",
        "           l2.append(0)\n",
        "    c = 0\n",
        "      \n",
        "   \n",
        "    for i in range(len(rvector)):\n",
        "        c+= l1[i]*l2[i]\n",
        "    l1 = np.array(l1)\n",
        "    l2 = np.array(l2)\n",
        "\n",
        "    cosine = c / float((np.sum(l1)*np.sum(l2))**0.5)\n",
        "    \n",
        "    if(cosine>max_sim):\n",
        "      maxsim_word=aspect\n",
        "  \n",
        "  aspects_with_cosinesim.append(maxsim_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGToTklBeRqA"
      },
      "outputs": [],
      "source": [
        "final_df_list_withsim=[]\n",
        "for i in range(len(sentences)):\n",
        "  temp=[reviewId[i],sentences[i],aspects_with_cosinesim[i],sentiments[i]]\n",
        "  final_df_list_withsim.append(temp)\n",
        "\n",
        "final_df_withsim=pd.DataFrame(final_df_list_withsim,columns=['ReviewId','sentence','aspect','sentiment score'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_withsim.head()"
      ],
      "metadata": {
        "id": "cKc8GsM6WFo0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "b3e84580-ac8c-40c4-bfe3-45e7e7abbfd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ReviewId                                           sentence       aspect  \\\n",
              "0         0  decided family holiday destination saw ranking...     location   \n",
              "1         0  plus staff extremely kind willing help time gr...      service   \n",
              "2         1  great customer service good restaurant service...      service   \n",
              "3         1  giulianuova pretty simple wow town clean beach...  cleanliness   \n",
              "4         1  baltic simple functional hotel make special pe...     location   \n",
              "\n",
              "   sentiment score  \n",
              "0         0.000000  \n",
              "1         0.211199  \n",
              "2         0.833333  \n",
              "3         0.179167  \n",
              "4         0.178571  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-05d6b1d5-048b-4959-8371-254361cc8902\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ReviewId</th>\n",
              "      <th>sentence</th>\n",
              "      <th>aspect</th>\n",
              "      <th>sentiment score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>decided family holiday destination saw ranking...</td>\n",
              "      <td>location</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>plus staff extremely kind willing help time gr...</td>\n",
              "      <td>service</td>\n",
              "      <td>0.211199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>great customer service good restaurant service...</td>\n",
              "      <td>service</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>giulianuova pretty simple wow town clean beach...</td>\n",
              "      <td>cleanliness</td>\n",
              "      <td>0.179167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>baltic simple functional hotel make special pe...</td>\n",
              "      <td>location</td>\n",
              "      <td>0.178571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05d6b1d5-048b-4959-8371-254361cc8902')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-05d6b1d5-048b-4959-8371-254361cc8902 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-05d6b1d5-048b-4959-8371-254361cc8902');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(final_df_withsim, \"drive/My Drive/Winter '22/IR/Project/final_df_withsim500K.sav\")"
      ],
      "metadata": {
        "id": "pCBrq-W_1BC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162ca11b-6684-4b24-8ecc-44d5290b9064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"drive/My Drive/Winter '22/IR/Project/final_df_withsim500K.sav\"]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#savedf\n",
        "final_df_withsim.to_csv('final_df_withsim500K.csv')\n",
        "!cp final_df_withsim.csv \"drive/My Drive/Winter '22/IR/Project/\""
      ],
      "metadata": {
        "id": "6nt-uXEQLz3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7847a1-c03c-47ca-db42-217ab42d97e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'final_df_withsim.csv': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"drive/My Drive/Winter '22/IR/Project/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfvqsImBHKwe",
        "outputId": "75611eab-0bec-4bc3-bbc6-561a107f80b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access \"drive/My Drive/Winter '22/IR/Project/\": No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "final_df_withsim = joblib.load(\"drive/My Drive/Winter '22/IR/Project/final_df_withsim500K.sav\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "3883q6bTHCQh",
        "outputId": "54bdff20-f9c5-42f6-9b60-156298854c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6443e3503eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinal_df_withsim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/My Drive/Winter '22/IR/Project/final_df_withsim500K.sav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"drive/My Drive/Winter '22/IR/Project/final_df_withsim500K.sav\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_withsim.head()"
      ],
      "metadata": {
        "id": "N2pyqgbvWHqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLXdew3BhuMS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( final_df_withsim['sentence'], final_df_withsim['aspect'], test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clearing memory\n",
        "final_df_withsim = None\n",
        "sentences= None\n",
        "aspects= None\n",
        "reviewId= None\n",
        "sentiments= None\n",
        "aspects_with_cosinesim= None"
      ],
      "metadata": {
        "id": "u_8IHrG7LbYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0GuGzA_qjL2"
      },
      "source": [
        "naive bayes (with cosine sim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuMBGMCmqoD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b801ad3b-7698-4af5-d71b-7b0eaf65fc04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.6253240239155732\n",
            "(0.8108021197734476, 0.43444976400082985, 0.4773614068370164, None)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(precision_recall_fscore_support(y_test, y_pred, average='macro'))\n",
        "#print(classification_report(y_test, y_pred,target_names=my_tags))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(nb, 'nb_sim.sav')"
      ],
      "metadata": {
        "id": "8Eyn1BYpSchP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c91df4-0c0a-42b7-9948-599cd674774d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nb_sim.sav']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(nb, \"drive/My Drive/Winter '22/IR/Project/nb_sim500K.sav\")"
      ],
      "metadata": {
        "id": "ipd5w55hrLtf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3cc244-eec9-4ca1-bc2b-4d34cf78e602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"drive/My Drive/Winter '22/IR/Project/nb_sim500K.sav\"]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb = joblib.load('nb_sim.sav')\n",
        "y_pred = nb.predict(X_test)\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))"
      ],
      "metadata": {
        "id": "8erYanwSS5WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "QmIu8UZtTsk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE4Z9H9LqfGE"
      },
      "source": [
        "SVM ( with cosine sim )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgM8JUe0dNuf"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', svm.SVC(decision_function_shape='ovr')),\n",
        "              ])\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "print(precision_recall_fscore_support(y_test, y_pred, average='macro'))\n",
        "#print(classification_report(y_test, y_pred,target_names=my_tags))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(nb, 'svm_sim.sav')"
      ],
      "metadata": {
        "id": "LJ0cKIKWTx-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(nb, \"drive/My Drive/Winter '22/IR/Project/svm_sim500K.sav\")"
      ],
      "metadata": {
        "id": "zZMSL0LyrYSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = joblib.load('svm_sim.sav')\n",
        "y_pred = nb.predict(X_test)\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))"
      ],
      "metadata": {
        "id": "wl5Q04fKTzD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngI5jzdVpBig"
      },
      "outputs": [],
      "source": [
        "nb.predict(pd.DataFrame([\" want clean room\"])[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree"
      ],
      "metadata": {
        "id": "3d0TM4sevP-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', DecisionTreeClassifier()),\n",
        "              ])\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "print(precision_recall_fscore_support(y_test, y_pred, average='macro'))\n",
        "#print(classification_report(y_test, y_pred,target_names=my_tags))"
      ],
      "metadata": {
        "id": "lX2tPtUMweqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb78376a-3eb3-4b68-9e4e-10d61eff2670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.6557682275992135\n",
            "(0.6427291481399131, 0.5819277171406937, 0.6072017530512868, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(nb, \"drive/My Drive/Winter '22/IR/Project/DT_sim500K.sav\")"
      ],
      "metadata": {
        "id": "LBGAiKC_vV6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a55c937-60e3-4bad-857c-f354adbebe0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"drive/My Drive/Winter '22/IR/Project/DT_sim500K.sav\"]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "GbxK6GYg3iwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', LogisticRegression()),\n",
        "              ])\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "print(precision_recall_fscore_support(y_test, y_pred, average='macro'))\n",
        "#print(classification_report(y_test, y_pred,target_names=my_tags))"
      ],
      "metadata": {
        "id": "6iqwPh6_16Rj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b4f20b-bf9d-4cdc-8b18-6df7f0fb8fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.700563781549697\n",
            "(0.8651317505579353, 0.5768825417167032, 0.655421842012111, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(nb, \"drive/My Drive/Winter '22/IR/Project/Logistic_sim500K.sav\")"
      ],
      "metadata": {
        "id": "TOvNCTd62Xmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f5f42a-8da3-473f-f7e6-83d008b3c389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"drive/My Drive/Winter '22/IR/Project/Logistic_sim500K.sav\"]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "Pb9go6Zq4x9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', KNeighborsClassifier(n_neighbors=10)),\n",
        "              ])\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "print(precision_recall_fscore_support(y_test, y_pred, average='macro'))\n",
        "#print(classification_report(y_test, y_pred,target_names=my_tags))"
      ],
      "metadata": {
        "id": "8TJr4tzS3yKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "156d1fd3-1e22-41de-dc01-c5f55e173902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.5529553388708318\n",
            "(0.6041064832056328, 0.40768966790619016, 0.44430691796412936, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(nb, \"drive/My Drive/Winter '22/IR/Project/KNN_sim500K.sav\")"
      ],
      "metadata": {
        "id": "hE288dOX4ENU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cafac9fd-4f38-4907-8ba6-86233ad1f1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"drive/My Drive/Winter '22/IR/Project/KNN_sim500K.sav\"]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "UOd2f2Tq4zsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', RandomForestClassifier(n_estimators=50, max_depth=80)),\n",
        "              ])\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "print(precision_recall_fscore_support(y_test, y_pred, average='macro'))\n",
        "#print(classification_report(y_test, y_pred,target_names=my_tags))"
      ],
      "metadata": {
        "id": "XYxvfA-W408u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2cbef02-6ca8-4234-f6d3-cb46050bf231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.5788692267565507\n",
            "(0.8701038481548389, 0.3678101423249022, 0.4005268475302578, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(nb, \"drive/My Drive/Winter '22/IR/Project/RForest_sim500K.sav\")"
      ],
      "metadata": {
        "id": "ZsJCnXOf6Eaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058d4d53-278c-49dc-8859-05b5a4e0982b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"drive/My Drive/Winter '22/IR/Project/RForest_sim500K.sav\"]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Testing_diff_models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}